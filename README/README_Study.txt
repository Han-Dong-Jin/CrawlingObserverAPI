25.03.08.
beautifulsoup과 request를 이용하여 간단한 크롤링 기법 구현(정적으로 구성된 news 페이지)

25.03.10 ~ 25.03.11.
구현한 크롤링 코드를 스케쥴러에 의해 작동되도록 변경
ABC 라이브러리를 이용하여 부모, 자식 클래스로 세분화하여 인터페이스 구현
main.py에서 각 라이브러리의 __init__을 호출함으로써 객체를 생성하는 방식으로 변경(OOP)

25.03.12. ~ 25.03.13.
각 크롤러가 스레드에서 실행되도록 변경
html 구조를 다시 분석하여 선택자 최적화
> 크롤링 할 각 요소를 따로 부르는 방식에서 큰 컨테이너를 먼저 부르고, 하위 요소들을 추출하는 방식으로 변경
애널리스트 리포트 데이터 크롤링 구현 완료

25.03.13.
html의 각 요소를 main에서 가져오는지, content에서 가져오는지 json 파일만 수정하면 유동적으로 작동하게 변경
스케쥴러도 json 파일로 관리하도록 변경
제무제표, 거시경제지표 크롤링 추가

25.03.17.
크롤링 데이터 tag 추가(DB에 넘길때 사용)
error fix

25.03.19.
주가 데이터 크롤링 추가

25.03.25.
각 크롤링 함수 데이터 구조 통일
traceback 예외처리 구현

25.03.26.
Secretary(DB저장 코드)추가
model 및 handler를 이용하여 구현

25.03.30.
크롤링 대상(선택자) 함수 구분에 핸들러 방식 적용
DB 기본키 오류 -> 복합키로 변경하여 보장
traceback 예외처리 방식을 전체 코드를 대상으로 변경
DB 저장시 rollback 오류 -> null값 가능성이 있는 데이터의 속성 변경
rollback 오류2 (외래키 제약조건) -> financial 데이터 insert시 flush로 바뀐 상태를 불러오지 않아서 외래키를 불러오지 못했음. 수정
DB 저장 코드 테스트 완료

25.04.01.
재무제표 데이터 DB 수정(프론트 요청)
not null 데이터 구분, 뷰 수정 / 필수 데이터 fallback 구현
traceback 예외처리 패키지 구분
기본키 생성 방식을 크롤링 데이터 해싱 방식으로 변경(중복 방지)
주가 데이터 symbol 압축 및 병렬처리 구현

25.04.05. ~ 25.04.06.
-- crawling DB
실시간성을 고려하여 주요 주식 종목은 1분, 나머지는 5분 15분으로 차등적으로 데이터를 크롤링
예외 코드를 미리 작성한 뒤 처리하는 방식으로 변경
-- assistAPIServer
백엔드 실전(controller service 구조, entity repository dto 구조)
change값 등 계산이 필요한 건 service 같이 로직이 실행되는 곳에서 처리 이해
dto 등은 데이터 형식만 지정하는 것 이해
전역 예외처리 및 예외타입 지정하여 프론트로 보내는 구조 이해
멀티 소스 기반에서 데이터를 어디서 가져오는지 지정을 안해서 에러가 났음
멀티 소스 기반 구조의 이해
-- github action
브랜치 보호규칙에 맞게 pull request 하는법(CI test 통과)
git 기록, 복구 등 git 명령어 학습
lint, pytest 조건에 따라서 실패한 request에 대한 시행착오
-- 협업 관련
프론트의 주식 종목 검색기능에 사용할 symbol - name 매칭 테이블의 요구를 반영하여 테이블 추가
주가 비율, 변화율 등 프론트에서 요청한 데이터를 처리하기 위해 크롤링 및 백엔드에 로직을 반영함
뷰 테이블에 프론트에서 필요한 데이터가 누락되어 해당 데이터를 추가하고, 뷰 구조를 재정의함
-- DB 관련
클라우드 지원이 시작되기 전에 로컬 서버에서 구축하기로 결정하여 assistAPIServer DB 구축을 시작함
데이터 insert시 uuid를 생성하고, binary 형식으로 변환하는 방식으로 메모리 및 성능의 향상을 기대할 수 있다는 것을 공부함
해당 내용을 반영하여 저장시에는 binary로, 뷰에는 다시 binary를 uuid로 변환하는 방식으로 구현함


예정
소켓 통신